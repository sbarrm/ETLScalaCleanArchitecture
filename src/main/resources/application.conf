# Snowflake connection section
connection {
  sfURL = "bk56294.west-europe.azure.snowflakecomputing.com"
  sfUser = "sparkuser"
  sfWarehouse = "COMPUTE_WH"
  sfRole = "ACCOUNTADMIN"
  # Note: sfPassword is not included here for security reasons. (add environment variable SF_PASSWORD with correct password to execute)
}

# Source table section with database and schema
source {
  sfDatabase = "SNOWFLAKE_SAMPLE_DATA"  # Source database
  sfSchema = "TPCH_SF1"                 # Source schema
  dbtable = "SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS"  # Source table
}

# Sink configuration section (where to store the results)
sink {
  sfDatabase = "TESTDATABASE"  # Target database
  sfSchema = "TESTSCHEMA"      # Target schema
  dbtable = "ORDERS_TEST"      # Target table
  writeMode = "overwrite"      # Write mode: overwrite, append, etc.
}

# Spark configuration section
spark {
  master = "local[*]"
  appName = "SnowflakeETLJob"
}
